---
description: Python Web Scraping Basics
globs: *.py
---

# Python Web Scraping Basics

Best practices and patterns for basic web scraping with Python.

<rule>
name: python_webscrape_basics
description: Guidelines for basic web scraping with Python
filters:
  - type: file_extension
    pattern: "\\.py$"
  - type: content
    pattern: "requests|BeautifulSoup|scrapy|selenium"

actions:
  - type: suggest
    conditions:
      - pattern: "import\\s+requests"
        message: "Consider adding proper error handling and timeout settings when using requests"
      
      - pattern: "requests\\.get\\([^,]+(\\))"
        message: "Add timeout parameter to requests.get() calls to prevent hanging on slow responses"
        
      - pattern: "(?<!with )requests\\.get"
        message: "Consider using 'with' context manager with requests to ensure proper resource cleanup"
        
      - pattern: "BeautifulSoup\\([^,]+(\\))"
        message: "Specify parser (e.g., 'html.parser', 'lxml') when initializing BeautifulSoup"
        
      - pattern: "time\\.sleep\\(\\d+\\)"
        message: "Consider using random delays between requests to be more respectful to servers"

examples:
  - input: |
      import requests
      
      def scrape_page(url):
          response = requests.get(url)
          return response.text
  - output: |
      import requests
      import random
      import time
      
      def scrape_page(url):
          try:
              with requests.get(url, timeout=10) as response:
                  response.raise_for_status()  # Raise exception for 4XX/5XX responses
                  time.sleep(random.uniform(1, 3))  # Be nice to the server
                  return response.text
          except requests.exceptions.RequestException as e:
              print(f"Error scraping {url}: {e}")
              return None

  - input: |
      from bs4 import BeautifulSoup
      
      def parse_html(html):
          soup = BeautifulSoup(html)
          return soup.find_all('div', class_='content')
  - output: |
      from bs4 import BeautifulSoup
      
      def parse_html(html):
          soup = BeautifulSoup(html, 'html.parser')  # Specify parser
          return soup.find_all('div', class_='content')

metadata:
  priority: high
  version: 1.0
</rule> 